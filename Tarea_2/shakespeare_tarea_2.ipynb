{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la Ciencia de Datos: Tarea 2\n",
    "\n",
    "Este notebook contiene el código de base para realizar la Tarea 2 del curso. Puede copiarlo en su propio repositorio y trabajar sobre el mismo.\n",
    "Las **instrucciones para ejecutar el notebook** están en la [página inicial del repositorio](https://gitlab.fing.edu.uy/maestria-cdaa/intro-cd/).\n",
    "\n",
    "**Se espera que no sea necesario revisar el código para corregir la tarea**, ya que todos los resultados y análisis relevantes deberían estar en el **informe en formato PDF**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar dependencias\n",
    "Para esta tarea, se han agregado algunos requerimientos, asegúrese de instalarlos (puede usar el mismo entorno virtual de la Tarea 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter pandas \"sqlalchemy<2.0\" pymysql seaborn pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexión a la Base y Lectura de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\") / \"shakespeare\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_table(table_name, engine):\n",
    "    \"\"\"\n",
    "    Leer la tabla con SQL y guardarla como CSV,\n",
    "    o cargarla desde el CSV si ya existe\n",
    "    \"\"\"\n",
    "    path_table = data_dir / f\"{table_name}.csv\"\n",
    "    if not path_table.exists():\n",
    "        print(f\"Consultando tabla con SQL: {table_name}\")\n",
    "        t0 = time()\n",
    "        df_table = pd.read_sql(f\"SELECT * FROM {table_name}\", engine)\n",
    "        t1 = time()\n",
    "        print(f\"Tiempo: {t1 - t0:.1f} segundos\")\n",
    "\n",
    "        print(f\"Guardando: {path_table}\\n\")\n",
    "        df_table.to_csv(path_table)\n",
    "    else:\n",
    "        print(f\"Cargando tabla desde CSV: {path_table}\")\n",
    "        df_table = pd.read_csv(path_table, index_col=[0])\n",
    "    return df_table\n",
    "\n",
    "\n",
    "print(\"Conectando a la base...\")\n",
    "conn_str = \"mysql+pymysql://guest:relational@relational.fit.cvut.cz:3306/Shakespeare\"\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Todos los párrafos de todas las obras\n",
    "df_paragraphs = load_table(\"paragraphs\", engine)\n",
    "\n",
    "df_characters = load_table(\"characters\", engine)\n",
    "\n",
    "df_works = load_table(\"works\", engine)\n",
    "\n",
    "df_chapters = load_table(\"chapters\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Actualizar con su versión de clean_text() de la Tarea_1\n",
    "\n",
    "def clean_text(df, column_name):\n",
    "    # Convertir todo a minúsculas\n",
    "    result = df[column_name].str.lower()\n",
    "\n",
    "    # FIXME: completar\n",
    "    for punc in [\"[\", \"\\n\", \",\"]:\n",
    "        result = result.str.replace(punc, \" \")\n",
    "    return result\n",
    "\n",
    "# Creamos una nueva columna CleanText a partir de PlainText\n",
    "df_paragraphs[\"CleanText\"] = clean_text(df_paragraphs, \"PlainText\")\n",
    "\n",
    "# Veamos la diferencia\n",
    "df_paragraphs[[\"PlainText\", \"CleanText\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos personajes, obras y géneros en el mismo dataset\n",
    "df_dataset = df_paragraphs.merge(df_chapters.set_index(\"id\")[\"work_id\"], left_on=\"chapter_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_works.set_index(\"id\")[[\"Title\", \"GenreType\"]], left_on=\"work_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_characters.set_index('id')[\"CharName\"], left_on=\"character_id\", right_index=True).sort_index()\n",
    "df_dataset = df_dataset[[\"CleanText\", \"CharName\", \"Title\", \"GenreType\"]]\n",
    "\n",
    "# Usaremos sólo estos personajes\n",
    "characters = [\"Antony\", \"Cleopatra\", \"Queen Margaret\"]\n",
    "df_dataset = df_dataset[df_dataset[\"CharName\"].isin(characters)]\n",
    "\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Párrafos por cada personaje seleccionado\n",
    "df_dataset[\"CharName\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y Features de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dataset[\"CleanText\"].to_numpy()\n",
    "y = df_dataset[\"CharName\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Partir train/test 30% estratificados\n",
    "# -> Definir X_train, X_test, y_train, y_test\n",
    "\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "print(f\"Tamaños de Train/Test: {len(X_train)}/{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de palabras y TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=None, ngram_range=(1,1))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfTransformer(use_idf=False)\n",
    "X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "X_train_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Realizar PCA sobre los datos de entrenamiento\n",
    "# reductor = ...\n",
    "\n",
    "# Transformar train\n",
    "X_train_red = reductor.fit_transform(X_train_tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las dos primeras componentes de PCA\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for character in np.unique(y_train):\n",
    "    mask_train = y_train == character\n",
    "    ax.scatter(X_train_red[mask_train, 0], X_train_red[mask_train, 1], label=character)\n",
    "\n",
    "ax.set_title(\"PCA por personaje\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bayes_clf = MultinomialNB().fit(X_train_tf, y_train)\n",
    "\n",
    "# Ver las primeras 10 predicciones de train\n",
    "y_pred_train = bayes_clf.predict(X_train_tf)\n",
    "y_pred_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "get_accuracy(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Predecir para test y ver la matriz de confusión, y reportar accuracy\n",
    "\n",
    "# X_test_counts = ...\n",
    "# X_test_tfidf = ...\n",
    "# y_test_pred = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de hiper-parámetros con Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# TODO: Agregar más variantes de parámetros que les parezcan relevantes\n",
    "param_sets = [{\"stop_words\": None, \"ngram\": (1,2), \"idf\": True},\n",
    "             {\"stop_words\": None, \"ngram\": (1,1), \"idf\": False}]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Ahora usaremos train/validation/test\n",
    "# Por lo tanto le renombramos train+validation = dev(elopment) dataset\n",
    "X_dev = X_train\n",
    "y_dev = y_train\n",
    "\n",
    "# # Para evitar errores\n",
    "# del X_train\n",
    "# del y_train\n",
    "\n",
    "for params in param_sets:\n",
    "    \n",
    "    # Transormaciones a aplicar (featurizers)\n",
    "    count_vect = CountVectorizer(stop_words=params[\"stop_words\"], ngram_range=params[\"ngram\"])\n",
    "    tf_idf = TfidfTransformer(use_idf=params[\"idf\"])\n",
    "    \n",
    "    for train_idxs, val_idxs in skf.split(X_dev, y_dev):\n",
    "        \n",
    "        # Train y validation para el split actual\n",
    "        X_train_ = X_dev[train_idxs]\n",
    "        y_train_ = y_dev[train_idxs]\n",
    "        X_val = X_dev[val_idxs]\n",
    "        y_val = y_dev[val_idxs]\n",
    "        \n",
    "        # Ajustamos y transformamos Train\n",
    "        X_train_counts = count_vect.fit_transform(X_train_)\n",
    "        X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "        \n",
    "        # TODO: Completar el código para entrenar y evaluar \n",
    "        \n",
    "        # Entrenamos con Train\n",
    "        # bayes_clf = ...\n",
    "\n",
    "        # Transformamos Validation\n",
    "        # X_val_counts = ...\n",
    "        # X_val_tfidf = ...\n",
    "        \n",
    "        # Predecimos y evaluamos en Validation\n",
    "        y_pred_val = bayes_clf.predict(X_val_tfidf)\n",
    "        acc = get_accuracy(y_val, y_pred_val)\n",
    "        print(f\"{acc=:.4f} {params=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Opcional) Comparativa con Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "y_train_s = np.char.replace(y_train.astype(str), \" \", \"_\").astype(object)\n",
    "y_test_s = np.char.replace(y_test.astype(str), \" \", \"_\").astype(object)\n",
    "\n",
    "# Convertimos al formato de fasttext: archivo de texto donde cada línea es:\n",
    "# __label__<label> TEXTO\n",
    "Xytrains = \"__label__\" + y_train_s.astype(object) + \" \" + X_train\n",
    "Xytests = \"__label__\" + y_test_s.astype(object) + \" \" + X_test\n",
    "np.savetxt(data_dir / \"train.txt\", Xytrains, fmt=\"%s\")\n",
    "np.savetxt(data_dir / \"test.txt\", Xytests, fmt=\"%s\")\n",
    "\n",
    "Xytests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=str(data_dir / \"train.txt\"), epoch=100, wordNgrams=2)\n",
    "model.test(str(data_dir / \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = model.predict(list(X_test))\n",
    "y_pred_test = [y[0].replace(\"__label__\", \"\") for y in y_out[0]]\n",
    "    \n",
    "print(get_accuracy(y_test_s, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
